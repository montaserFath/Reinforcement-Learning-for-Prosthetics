{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed_daud_wd/CondaEnvs/opensimEnv_V2/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    }
   ],
   "source": [
    "# Import helper libraries\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "#__________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "# Import scientific libraries\n",
    "\n",
    "import numpy as np               # Numpy, a good library to deal with matrices in python.\n",
    "import matplotlib.pyplot as plt  # Matplotlib, a good library for plotting in python.\n",
    "from matplotlib import style\n",
    "#__________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "import gym                       # Gym, a collection of RL environemnts.\n",
    "gym.undo_logger_setup()  # NOQA\n",
    "from gym import spaces  \n",
    "import gym.wrappers\n",
    "\n",
    "#__________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "import chainer                               # Chainer, a python-based deep learning framework. Chainerrl, a reinforcement learning library based on chainer framework.\n",
    "from chainer import optimizers               # a collection of Neural Network optimizers.\n",
    "from chainerrl.agents.ddpg import DDPG       # a DDPG agent\n",
    "from chainerrl.agents.ddpg import DDPGModel  # a DDPG model, responsibles to combine the policy network and the value function network.\n",
    "from chainerrl import explorers              # a collection of explores functions.\n",
    "from chainerrl import misc                   # a collection of utility functions to manipulate the environemnts.\n",
    "from chainerrl import policy                 # a policy network\n",
    "from chainerrl import q_functions            # a value function network\n",
    "from chainerrl import replay_buffer          # a Replay buffer to store a set of observations for the DDPG agent.\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment settings\n",
    "\n",
    "#env_name='BipedalWalker-v2'\n",
    "env_name='Pendulum-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer's settings\n",
    "seed=0\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for Policy and Value function Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: 1\n",
    "\n",
    "\n",
    "# Policy Network ( Actor ).\n",
    "\n",
    "actor_hidden_layers=3                        # Number of hidden layers.\n",
    "actor_hidden_units=300                       # Number of hidden units\n",
    "actor_lr=1e-4                                # learning rate\n",
    "\n",
    "# Value function Network ( Critic )\n",
    "critic_hidden_layers=3                       # Number of hidden layers.\n",
    "critic_hidden_units=300                      # Number of hidden units\n",
    "critic_lr=1e-3                               # learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: 2\n",
    "\n",
    "number_of_episodes=2000                     # Number of episodes\n",
    "max_episode_length=1000                     # Max length of the episode.\n",
    "\n",
    "replay_buffer_size=5 * 10 ** 5              # the size of the replay buffer.\n",
    "replay_start_size=5000                      # the size of the replay buffer when the network starts the training step.\n",
    "number_of_update_times=1                    # Number of repetition of update.\n",
    "\n",
    "target_update_interval=1                    # Target update interval in each step.\n",
    "target_update_method='soft'                 # the type of update: hard or soft.\n",
    "\n",
    "soft_update_tau=1e-2                        # The value of Tau  in the soft target update.\n",
    "update_interval=4                           # Model update interval in eac step.\n",
    "number_of_eval_runs=100\n",
    "# eval_interval=10 ** 5\n",
    "\n",
    "# final_exploration_steps=10 ** 6            \n",
    "\n",
    "gamma=0.995                               # Discount factor\n",
    "minibatch_size=128                        # Batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A set of helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    \"\"\" limit the an action value between the higest and lowest values in action space.\n",
    "    Input: a\n",
    "    Output: clipped action\n",
    "    \"\"\"\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):\n",
    "    \"\"\" Scale the reward value.\n",
    "    Input: reward (r)\n",
    "    Output: scaled reward\n",
    "    \"\"\"\n",
    "    return r *1 #1e-2\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    \"\"\" Convert the data type of the observation to float-32\n",
    "    Input: observation (obs)\n",
    "    Output:  the processed observation \n",
    "    \"\"\" \n",
    "    obs=np.array(obs)\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "def random_action():\n",
    "    \"\"\" Generate a random action.\n",
    "    Input: None\n",
    "    Output:  a random action\n",
    "    \"\"\" \n",
    "    a = action_space.sample()\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.astype(np.float32)\n",
    "    return a\n",
    "\n",
    "\n",
    "def make_env(test,env_name,render=False):\n",
    "     \"\"\" Create an instance from \"CartPole\" environment\n",
    "    Input: a boolean value to show if it's an agent training experiment or test experiment (test)\n",
    "    Output:  \"CartPole\" environment (env)\n",
    "    \"\"\" \n",
    "        \n",
    "    env = gym.make(env_name)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "    #if args.monitor:\n",
    "        #env = gym.wrappers.Monitor(env, args.outdir)\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        misc.env_modifiers.make_action_filtered(env, clip_action_filter)\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = make_env(test=False,env_name=env_name,render=False)\n",
    "timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "obs_size = np.asarray(env.observation_space.shape).prod()\n",
    "action_space = env.action_space\n",
    "\n",
    "action_size = np.asarray(action_space.shape).prod()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the \"Actor\" and \"Value function\" Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "\n",
    "q_func = q_functions.FCSAQFunction(\n",
    "            obs_size, \n",
    "            action_size,\n",
    "            n_hidden_channels=critic_hidden_units,\n",
    "            n_hidden_layers=critic_hidden_layers)\n",
    "\n",
    "# policy Network\n",
    "\n",
    "pi = policy.FCDeterministicPolicy(\n",
    "            obs_size, \n",
    "            action_size=action_size,\n",
    "            n_hidden_channels=actor_hidden_units,\n",
    "            n_hidden_layers=actor_hidden_layers,\n",
    "            min_action=action_space.low, \n",
    "            max_action=action_space.high,\n",
    "            bound_action=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the \"DDPG\" agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "\n",
    "model = DDPGModel(q_func=q_func, policy=pi)\n",
    "opt_actor = optimizers.Adam(alpha=actor_lr)\n",
    "opt_critic = optimizers.Adam(alpha=critic_lr)\n",
    "opt_actor.setup(model['policy'])\n",
    "opt_critic.setup(model['q_function'])\n",
    "opt_actor.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_a')\n",
    "opt_critic.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_c')\n",
    "\n",
    "rbuf = replay_buffer.ReplayBuffer(replay_buffer_size)\n",
    "ou_sigma = (action_space.high - action_space.low) * 0.2\n",
    "\n",
    "explorer = explorers.AdditiveOU(sigma=ou_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent\n",
    "agent = DDPG(model, opt_actor, opt_critic, rbuf, gamma=gamma,\n",
    "                 explorer=explorer, replay_start_size=replay_start_size,\n",
    "                 target_update_method=target_update_method,\n",
    "                 target_update_interval=target_update_interval,\n",
    "                 update_interval=update_interval,\n",
    "                 soft_update_tau=soft_update_tau,\n",
    "                 n_times_update=number_of_update_times,\n",
    "                 phi=phi,minibatch_size=minibatch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  10\n",
      "Rewards:  -1420.1520712496447\n",
      "Max reward so far:  -1035.1203805818989\n",
      "Mean Reward -1192.7269671117201\n",
      "==========================================\n",
      "Episode:  20\n",
      "Rewards:  -1149.2969592483244\n",
      "Max reward so far:  -959.8272729705341\n",
      "Mean Reward -1226.1514915300652\n",
      "==========================================\n",
      "Episode:  30\n",
      "Rewards:  -1533.982744557468\n",
      "Max reward so far:  -959.8272729705341\n",
      "Mean Reward -1253.7982974671856\n",
      "==========================================\n",
      "Episode:  40\n",
      "Rewards:  -1160.1434068889396\n",
      "Max reward so far:  -959.8272729705341\n",
      "Mean Reward -1286.0403882683024\n",
      "==========================================\n",
      "Episode:  50\n",
      "Rewards:  -890.2554837456627\n",
      "Max reward so far:  -890.2554837456627\n",
      "Mean Reward -1268.5837102460523\n",
      "==========================================\n",
      "Episode:  60\n",
      "Rewards:  -1100.3916348131195\n",
      "Max reward so far:  -890.2554837456627\n",
      "Mean Reward -1276.4830958213768\n",
      "==========================================\n",
      "Episode:  70\n",
      "Rewards:  -1211.5650274173609\n",
      "Max reward so far:  -890.2554837456627\n",
      "Mean Reward -1259.9717898043377\n",
      "==========================================\n",
      "Episode:  80\n",
      "Rewards:  -1206.5719342005498\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1244.8458231802724\n",
      "==========================================\n",
      "Episode:  90\n",
      "Rewards:  -1250.815086093162\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1248.1630350113207\n",
      "==========================================\n",
      "Episode:  100\n",
      "Rewards:  -1403.486255458664\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1254.694560397882\n",
      "==========================================\n",
      "Episode:  110\n",
      "Rewards:  -1479.2242607869437\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1262.7320204059372\n",
      "==========================================\n",
      "Episode:  120\n",
      "Rewards:  -1085.0176215762415\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1257.4503259237279\n",
      "==========================================\n",
      "Episode:  130\n",
      "Rewards:  -1582.83886869557\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1260.4604804708729\n",
      "==========================================\n",
      "Episode:  140\n",
      "Rewards:  -1342.2164664314816\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1261.0557808726423\n",
      "==========================================\n",
      "Episode:  150\n",
      "Rewards:  -1514.1346515212706\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1262.7247563605872\n",
      "==========================================\n",
      "Episode:  160\n",
      "Rewards:  -1384.7736936599692\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1266.2083138951712\n",
      "==========================================\n",
      "Episode:  170\n",
      "Rewards:  -1101.7882322704534\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1263.8914708687394\n",
      "==========================================\n",
      "Episode:  180\n",
      "Rewards:  -1068.1357056518532\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1265.0002370367108\n",
      "==========================================\n",
      "Episode:  190\n",
      "Rewards:  -1264.6064663218244\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1265.0371594841617\n",
      "==========================================\n",
      "Episode:  200\n",
      "Rewards:  -1277.5100281586792\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1273.9059759982815\n",
      "==========================================\n",
      "Episode:  210\n",
      "Rewards:  -1052.9329910700542\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1270.3710568300237\n",
      "==========================================\n",
      "Episode:  220\n",
      "Rewards:  -1624.8810711631143\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1270.8768553696732\n",
      "==========================================\n",
      "Episode:  230\n",
      "Rewards:  -1191.195914731334\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1273.638417933123\n",
      "==========================================\n",
      "Episode:  240\n",
      "Rewards:  -1497.8438948484863\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1274.862859705885\n",
      "==========================================\n",
      "Episode:  250\n",
      "Rewards:  -1086.2298708519238\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1274.3744798624077\n",
      "==========================================\n",
      "Episode:  260\n",
      "Rewards:  -889.2268000673823\n",
      "Max reward so far:  -778.9234707114813\n",
      "Mean Reward -1270.7578826857225\n",
      "==========================================\n",
      "Episode:  270\n",
      "Rewards:  -506.5434896086269\n",
      "Max reward so far:  -506.5434896086269\n",
      "Mean Reward -1252.7479896837317\n",
      "==========================================\n",
      "Episode:  280\n",
      "Rewards:  -628.6841662209936\n",
      "Max reward so far:  -373.226855026076\n",
      "Mean Reward -1233.4389041592092\n",
      "==========================================\n",
      "Episode:  290\n",
      "Rewards:  -739.5309844965208\n",
      "Max reward so far:  -372.0065394624916\n",
      "Mean Reward -1212.421077620452\n",
      "==========================================\n",
      "Episode:  300\n",
      "Rewards:  -610.926876468846\n",
      "Max reward so far:  -356.2976532955723\n",
      "Mean Reward -1191.8297403123222\n",
      "==========================================\n",
      "Episode:  310\n",
      "Rewards:  -491.5903625941692\n",
      "Max reward so far:  -356.2976532955723\n",
      "Mean Reward -1172.3853929512452\n",
      "==========================================\n",
      "Episode:  320\n",
      "Rewards:  -583.1666941380877\n",
      "Max reward so far:  -356.2976532955723\n",
      "Mean Reward -1159.0646651383918\n",
      "==========================================\n",
      "Episode:  330\n",
      "Rewards:  -380.4298740487102\n",
      "Max reward so far:  -356.2976532955723\n",
      "Mean Reward -1144.268955770976\n",
      "==========================================\n",
      "Episode:  340\n",
      "Rewards:  -865.329437936257\n",
      "Max reward so far:  -356.2976532955723\n",
      "Mean Reward -1127.562744557232\n"
     ]
    }
   ],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"DDPG_Pendulum_10000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        #env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"DDPG_Pendulum_reward_10000.txt\", \"a\"))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"DDPG_Pendulum_MEAN_Reward_10000.txt\", \"a\"))    \n",
    "                \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')\n",
    "\n",
    "\n",
    "plt.xlabel('episdes')\n",
    "plt.ylabel('reword')    \n",
    "plt.plot(G)   \n",
    "plt.savefig('DDPG_Pendulum_10000episodes.png',dpi=1000)\n",
    "\n",
    "\n",
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns')\n",
    "plt.xlabel('Number of episodes/10')\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes DDPG_Pendulum_10000\",dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"DDPG_Pendulum_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"Returns_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenSimEnv_V2)",
   "language": "python",
   "name": "opensimenv_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
