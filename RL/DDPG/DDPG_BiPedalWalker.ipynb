{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed_daud_wd/CondaEnvs/opensimEnv_V2/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    }
   ],
   "source": [
    "# Import helper libraries\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "#__________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "# Import scientific libraries\n",
    "\n",
    "import numpy as np               # Numpy, a good library to deal with matrices in python.\n",
    "import matplotlib.pyplot as plt  # Matplotlib, a good library for plotting in python.\n",
    "from matplotlib import style\n",
    "#__________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "import gym                       # Gym, a collection of RL environemnts.\n",
    "gym.undo_logger_setup()  # NOQA\n",
    "from gym import spaces  \n",
    "import gym.wrappers\n",
    "\n",
    "#__________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "import chainer                               # Chainer, a python-based deep learning framework. Chainerrl, a reinforcement learning library based on chainer framework.\n",
    "from chainer import optimizers               # a collection of Neural Network optimizers.\n",
    "from chainerrl.agents.ddpg import DDPG       # a DDPG agent\n",
    "from chainerrl.agents.ddpg import DDPGModel  # a DDPG model, responsibles to combine the policy network and the value function network.\n",
    "from chainerrl import explorers              # a collection of explores functions.\n",
    "from chainerrl import misc                   # a collection of utility functions to manipulate the environemnts.\n",
    "from chainerrl import policy                 # a policy network\n",
    "from chainerrl import q_functions            # a value function network\n",
    "from chainerrl import replay_buffer          # a Replay buffer to store a set of observations for the DDPG agent.\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment settings\n",
    "\n",
    "env_name='BipedalWalker-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer's settings\n",
    "seed=0\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for Policy and Value function Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: 1\n",
    "\n",
    "\n",
    "# Policy Network ( Actor ).\n",
    "\n",
    "actor_hidden_layers=3                        # Number of hidden layers.\n",
    "actor_hidden_units=300                       # Number of hidden units\n",
    "actor_lr=1e-4                                # learning rate\n",
    "\n",
    "# Value function Network ( Critic )\n",
    "critic_hidden_layers=3                       # Number of hidden layers.\n",
    "critic_hidden_units=300                      # Number of hidden units\n",
    "critic_lr=1e-3                               # learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: 2\n",
    "\n",
    "number_of_episodes=2000                     # Number of episodes\n",
    "max_episode_length=1000                     # Max length of the episode.\n",
    "\n",
    "replay_buffer_size=5 * 10 ** 5              # the size of the replay buffer.\n",
    "replay_start_size=5000                      # the size of the replay buffer when the network starts the training step.\n",
    "number_of_update_times=1                    # Number of repetition of update.\n",
    "\n",
    "target_update_interval=1                    # Target update interval in each step.\n",
    "target_update_method='soft'                 # the type of update: hard or soft.\n",
    "\n",
    "soft_update_tau=1e-2                        # The value of Tau  in the soft target update.\n",
    "update_interval=4                           # Model update interval in eac step.\n",
    "number_of_eval_runs=100\n",
    "# eval_interval=10 ** 5\n",
    "\n",
    "# final_exploration_steps=10 ** 6            \n",
    "\n",
    "gamma=0.995                               # Discount factor\n",
    "minibatch_size=128                        # Batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A set of helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    \"\"\" limit the an action value between the higest and lowest values in action space.\n",
    "    Input: a\n",
    "    Output: clipped action\n",
    "    \"\"\"\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):\n",
    "    \"\"\" Scale the reward value.\n",
    "    Input: reward (r)\n",
    "    Output: scaled reward\n",
    "    \"\"\"\n",
    "    return r *1 #1e-2\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    \"\"\" Convert the data type of the observation to float-32\n",
    "    Input: observation (obs)\n",
    "    Output:  the processed observation \n",
    "    \"\"\" \n",
    "    obs=np.array(obs)\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "def random_action():\n",
    "    \"\"\" Generate a random action.\n",
    "    Input: None\n",
    "    Output:  a random action\n",
    "    \"\"\" \n",
    "    a = action_space.sample()\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = a.astype(np.float32)\n",
    "    return a\n",
    "\n",
    "\n",
    "def make_env(test,env_name,render=False):\n",
    "     \"\"\" Create an instance from \"CartPole\" environment\n",
    "    Input: a boolean value to show if it's an agent training experiment or test experiment (test)\n",
    "    Output:  \"CartPole\" environment (env)\n",
    "    \"\"\" \n",
    "        \n",
    "    env = gym.make(env_name)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "    #if args.monitor:\n",
    "        #env = gym.wrappers.Monitor(env, args.outdir)\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        misc.env_modifiers.make_action_filtered(env, clip_action_filter)\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = make_env(test=False,env_name=env_name,render=False)\n",
    "timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "obs_size = np.asarray(env.observation_space.shape).prod()\n",
    "action_space = env.action_space\n",
    "\n",
    "action_size = np.asarray(action_space.shape).prod()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the \"Actor\" and \"Value function\" Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "\n",
    "q_func = q_functions.FCSAQFunction(\n",
    "            obs_size, \n",
    "            action_size,\n",
    "            n_hidden_channels=critic_hidden_units,\n",
    "            n_hidden_layers=critic_hidden_layers)\n",
    "\n",
    "# policy Network\n",
    "\n",
    "pi = policy.FCDeterministicPolicy(\n",
    "            obs_size, \n",
    "            action_size=action_size,\n",
    "            n_hidden_channels=actor_hidden_units,\n",
    "            n_hidden_layers=actor_hidden_layers,\n",
    "            min_action=action_space.low, \n",
    "            max_action=action_space.high,\n",
    "            bound_action=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the \"DDPG\" agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "\n",
    "model = DDPGModel(q_func=q_func, policy=pi)\n",
    "opt_actor = optimizers.Adam(alpha=actor_lr)\n",
    "opt_critic = optimizers.Adam(alpha=critic_lr)\n",
    "opt_actor.setup(model['policy'])\n",
    "opt_critic.setup(model['q_function'])\n",
    "opt_actor.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_a')\n",
    "opt_critic.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_c')\n",
    "\n",
    "rbuf = replay_buffer.ReplayBuffer(replay_buffer_size)\n",
    "ou_sigma = (action_space.high - action_space.low) * 0.2\n",
    "\n",
    "explorer = explorers.AdditiveOU(sigma=ou_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent\n",
    "agent = DDPG(model, opt_actor, opt_critic, rbuf, gamma=gamma,\n",
    "                 explorer=explorer, replay_start_size=replay_start_size,\n",
    "                 target_update_method=target_update_method,\n",
    "                 target_update_interval=target_update_interval,\n",
    "                 update_interval=update_interval,\n",
    "                 soft_update_tau=soft_update_tau,\n",
    "                 n_times_update=number_of_update_times,\n",
    "                 phi=phi,minibatch_size=minibatch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  10\n",
      "Rewards:  -115.8567079978995\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -104.4450365851455\n",
      "==========================================\n",
      "Episode:  20\n",
      "Rewards:  -116.30052945398725\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -108.72524299432676\n",
      "==========================================\n",
      "Episode:  30\n",
      "Rewards:  -117.26406881289184\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -111.13985947027953\n",
      "==========================================\n",
      "Episode:  40\n",
      "Rewards:  -114.3030295380832\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -111.99672982898147\n",
      "==========================================\n",
      "Episode:  50\n",
      "Rewards:  -139.77679149293527\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -113.16340688603006\n",
      "==========================================\n",
      "Episode:  60\n",
      "Rewards:  -115.6242822906077\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -113.36392853880514\n",
      "==========================================\n",
      "Episode:  70\n",
      "Rewards:  -113.420904454402\n",
      "Max reward so far:  -99.36613102459783\n",
      "Mean Reward -112.97048228973958\n",
      "==========================================\n",
      "Episode:  80\n",
      "Rewards:  -116.60494609495564\n",
      "Max reward so far:  -97.26116805853943\n",
      "Mean Reward -113.19439347432544\n",
      "==========================================\n",
      "Episode:  90\n",
      "Rewards:  -115.45811605728605\n",
      "Max reward so far:  -97.26116805853943\n",
      "Mean Reward -113.15817838680199\n",
      "==========================================\n",
      "Episode:  100\n",
      "Rewards:  -31.33259664347088\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -112.69039758448743\n",
      "==========================================\n",
      "Episode:  110\n",
      "Rewards:  -124.20055276906987\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.00757177225934\n",
      "==========================================\n",
      "Episode:  120\n",
      "Rewards:  -126.77140345291296\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.44563009988809\n",
      "==========================================\n",
      "Episode:  130\n",
      "Rewards:  -114.5867968087159\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.43771332776475\n",
      "==========================================\n",
      "Episode:  140\n",
      "Rewards:  -119.68853673172929\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.52291632956934\n",
      "==========================================\n",
      "Episode:  150\n",
      "Rewards:  -118.30809609614872\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.68082817291554\n",
      "==========================================\n",
      "Episode:  160\n",
      "Rewards:  -111.3014928765508\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.64255277496284\n",
      "==========================================\n",
      "Episode:  170\n",
      "Rewards:  -118.09837156570889\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.67731673347278\n",
      "==========================================\n",
      "Episode:  180\n",
      "Rewards:  -106.2670202253914\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.63446523873596\n",
      "==========================================\n",
      "Episode:  190\n",
      "Rewards:  -115.90770251683084\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.91636593149138\n",
      "==========================================\n",
      "Episode:  200\n",
      "Rewards:  -114.08354447274283\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.00198070796793\n",
      "==========================================\n",
      "Episode:  210\n",
      "Rewards:  -111.19713517421981\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.88271119683172\n",
      "==========================================\n",
      "Episode:  220\n",
      "Rewards:  -132.68465238174548\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -113.99908932051271\n",
      "==========================================\n",
      "Episode:  230\n",
      "Rewards:  -105.44085757796032\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.09131625236687\n",
      "==========================================\n",
      "Episode:  240\n",
      "Rewards:  -132.21310814891694\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.15797261911824\n",
      "==========================================\n",
      "Episode:  250\n",
      "Rewards:  -127.11885565675422\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.26901652663558\n",
      "==========================================\n",
      "Episode:  260\n",
      "Rewards:  -118.22869849932752\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.38443617151731\n",
      "==========================================\n",
      "Episode:  270\n",
      "Rewards:  -112.65967043116751\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.69056191808959\n",
      "==========================================\n",
      "Episode:  280\n",
      "Rewards:  -114.20493240654655\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.72276340691963\n",
      "==========================================\n",
      "Episode:  290\n",
      "Rewards:  -127.38607375766088\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.82221909867481\n",
      "==========================================\n",
      "Episode:  300\n",
      "Rewards:  -117.74083410133919\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.67289704037492\n",
      "==========================================\n",
      "Episode:  310\n",
      "Rewards:  -119.96448366764933\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.62142055983207\n",
      "==========================================\n",
      "Episode:  320\n",
      "Rewards:  -116.67750092816415\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.6436162107747\n",
      "==========================================\n",
      "Episode:  330\n",
      "Rewards:  -120.67376588768884\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.79425642142937\n",
      "==========================================\n",
      "Episode:  340\n",
      "Rewards:  -114.94637148646886\n",
      "Max reward so far:  -31.33259664347088\n",
      "Mean Reward -114.76543875465323\n",
      "==========================================\n",
      "Episode:  350\n",
      "Rewards:  -111.31504513684226\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.57033909268509\n",
      "==========================================\n",
      "Episode:  360\n",
      "Rewards:  -125.83114784578794\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.64099521229093\n",
      "==========================================\n",
      "Episode:  370\n",
      "Rewards:  -122.17215255922949\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.67376712881541\n",
      "==========================================\n",
      "Episode:  380\n",
      "Rewards:  -119.5480675100175\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.56931018671739\n",
      "==========================================\n",
      "Episode:  390\n",
      "Rewards:  -110.85851889333439\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.61462112677232\n",
      "==========================================\n",
      "Episode:  400\n",
      "Rewards:  -108.6116478331089\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.65533990071202\n",
      "==========================================\n",
      "Episode:  410\n",
      "Rewards:  -121.72390101982032\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.6590667251667\n",
      "==========================================\n",
      "Episode:  420\n",
      "Rewards:  -111.77414213861587\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.55712367143578\n",
      "==========================================\n",
      "Episode:  430\n",
      "Rewards:  -115.34675224344122\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.47833865668751\n",
      "==========================================\n",
      "Episode:  440\n",
      "Rewards:  -111.01645477890285\n",
      "Max reward so far:  -16.5494499654056\n",
      "Mean Reward -114.25607821924116\n"
     ]
    }
   ],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"DDPG_Walker2D_10000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        #env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"DDPG_Walker2D_reward_10000.txt\", \"a\"))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"DDPG_Walker2D_MEAN_Reward_10000.txt\", \"a\"))    \n",
    "                \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"DDPG_BiPedalWalker_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"Returns_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenSimEnv_V2)",
   "language": "python",
   "name": "opensimenv_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
