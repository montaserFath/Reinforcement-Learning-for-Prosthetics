{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/waleed_daud_wd/CondaEnvs/opensimEnv_V2/lib/python3.6/site-packages/gym/__init__.py:15: UserWarning: gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\n",
      "  warnings.warn(\"gym.undo_logger_setup is deprecated. gym no longer modifies the global logging configuration\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import absolute_import\n",
    "from builtins import *  # NOQA\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import gym\n",
    "gym.undo_logger_setup()  # NOQA\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "from osim.env import ProstheticsEnv\n",
    "\n",
    "\n",
    "from chainerrl.agents import a3c\n",
    "from chainerrl.agents import PPO\n",
    "from chainerrl import experiments\n",
    "from chainerrl import links\n",
    "from chainerrl import misc\n",
    "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
    "from chainerrl import policies\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer's settings\n",
    "seed=1\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Setting\n",
    "\n",
    "#actor_hidden_layers=3\n",
    "#actor_hidden_units=300\n",
    "actor_lr=1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other settings\n",
    "\n",
    "number_of_episodes=5000\n",
    "max_episode_length=1000\n",
    "\n",
    "update_interval=4\n",
    "\n",
    "number_of_eval_runs=10\n",
    "eval_interval=10 ** 4\n",
    "\n",
    "epochs=10\n",
    "gamma=0.995\n",
    "batch_size=128\n",
    "entropy_coef=0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers classes\n",
    "\n",
    "\n",
    "class A3CFFSoftmax(chainer.ChainList, a3c.A3CModel):\n",
    "    \"\"\"An example of A3C feedforward softmax policy.\"\"\"\n",
    "\n",
    "    def __init__(self, ndim_obs, n_actions, hidden_sizes=(200, 200)):\n",
    "        self.pi = policies.SoftmaxPolicy(\n",
    "            model=links.MLP(ndim_obs, n_actions, hidden_sizes))\n",
    "        self.v = links.MLP(ndim_obs, 1, hidden_sizes=hidden_sizes)\n",
    "        super().__init__(self.pi, self.v)\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        return self.pi(state), self.v(state)\n",
    "\n",
    "\n",
    "class A3CFFMellowmax(chainer.ChainList, a3c.A3CModel):\n",
    "    \"\"\"An example of A3C feedforward mellowmax policy.\"\"\"\n",
    "\n",
    "    def __init__(self, ndim_obs, n_actions, hidden_sizes=(200, 200)):\n",
    "        self.pi = policies.MellowmaxPolicy(\n",
    "            model=links.MLP(ndim_obs, n_actions, hidden_sizes))\n",
    "        self.v = links.MLP(ndim_obs, 1, hidden_sizes=hidden_sizes)\n",
    "        super().__init__(self.pi, self.v)\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        return self.pi(state), self.v(state)\n",
    "\n",
    "\n",
    "class A3CFFGaussian(chainer.Chain, a3c.A3CModel):\n",
    "    \"\"\"An example of A3C feedforward Gaussian policy.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, action_space,\n",
    "                 n_hidden_layers=2, n_hidden_channels=64,\n",
    "                 bound_mean=None, normalize_obs=None):\n",
    "        assert bound_mean in [False, True]\n",
    "        assert normalize_obs in [False, True]\n",
    "        super().__init__()\n",
    "        hidden_sizes = (n_hidden_channels,) * n_hidden_layers\n",
    "        self.normalize_obs = normalize_obs\n",
    "        with self.init_scope():\n",
    "            self.pi = policies.FCGaussianPolicyWithStateIndependentCovariance(\n",
    "                obs_size, action_space.low.size,\n",
    "                n_hidden_layers, n_hidden_channels,\n",
    "                var_type='diagonal', nonlinearity=F.tanh,\n",
    "                bound_mean=bound_mean,\n",
    "                min_action=action_space.low, max_action=action_space.high,\n",
    "                mean_wscale=1e-2)\n",
    "            self.v = links.MLP(obs_size, 1, hidden_sizes=hidden_sizes)\n",
    "            if self.normalize_obs:\n",
    "                self.obs_filter = links.EmpiricalNormalization(\n",
    "                    shape=obs_size\n",
    "                )\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        if self.normalize_obs:\n",
    "            state = F.clip(self.obs_filter(state, update=False),\n",
    "                           -5.0, 5.0)\n",
    "\n",
    "        return self.pi(state), self.v(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper's functions\n",
    "\n",
    "# Linearly decay the learning rate to zero\n",
    "def lr_setter(env, agent, value):\n",
    "    agent.optimizer.alpha = value\n",
    "\n",
    "# Linearly decay the clipping parameter to zero\n",
    "def clip_eps_setter(env, agent, value):\n",
    "    agent.clip_eps = value\n",
    "\n",
    "\n",
    "def clip_action_filter(a):\n",
    "    return np.clip(a, action_space.low, action_space.high)\n",
    "\n",
    "def reward_filter(r):\n",
    "    return r * 1\n",
    "\n",
    "\n",
    "def phi(obs):\n",
    "    obs=np.array(obs)\n",
    "    return obs.astype(np.float32)\n",
    "\n",
    "\n",
    "def make_env(test,render=False):\n",
    "    env = ProstheticsEnv(visualize=render)\n",
    "    # Use different random seeds for train and test envs\n",
    "    env_seed = 2 ** 32 - 1 - seed if test else seed\n",
    "    env.seed(env_seed)\n",
    "\n",
    "    if not test:\n",
    "        misc.env_modifiers.make_reward_filtered(env, reward_filter)\n",
    "    if render and not test:\n",
    "        misc.env_modifiers.make_rendered(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: Could not seed environment <ProstheticsEnv<0>>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = make_env(test=False,render=False)\n",
    "obs_space=env.observation_space\n",
    "obs_size = obs_space.low.size\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A3CFFGaussian(obs_size, action_space,bound_mean=True,normalize_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x7f60039fe898>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = chainer.optimizers.Adam(alpha=actor_lr, eps=1e-5)\n",
    "opt.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(model, opt,\n",
    "                #gpu=args.gpu,\n",
    "                phi=phi,\n",
    "                update_interval=update_interval,\n",
    "                minibatch_size=batch_size, epochs=epochs,\n",
    "                clip_eps_vf=None, entropy_coef=entropy_coef,\n",
    "                #standardize_advantages=args.standardize_advantages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: Could not seed environment <ProstheticsEnv<0>>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#lr_decay_hook = experiments.LinearInterpolationHook(number_of_steps, actor_lr, 0, lr_setter)\n",
    "\n",
    "#clip_eps_decay_hook = experiments.LinearInterpolationHook(number_of_steps, 0.2, 0, clip_eps_setter)\n",
    "\n",
    "eval_env = make_env(test=True,render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  10\n",
      "Rewards:  -540.4483801846826\n",
      "Max reward so far:  -525.5893674184133\n",
      "Mean Reward -491.9189284847414\n",
      "==========================================\n",
      "Episode:  20\n",
      "Rewards:  -539.7766174618978\n",
      "Max reward so far:  -200.8862833155811\n",
      "Mean Reward -486.6985488763451\n"
     ]
    }
   ],
   "source": [
    "G=[]\n",
    "G_mean=[]\n",
    "for ep in range(1, number_of_episodes+ 1):\n",
    "    if ep%100:\n",
    "        agent.save(\"PPO_Prosthetic_5000\")\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    episode_rewards=[]\n",
    "    while not done and t < max_episode_length:\n",
    "        # Uncomment to watch the behaviour\n",
    "        #env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        episode_rewards.append(reward)\n",
    "        t += 1\n",
    "        \n",
    "    if done or t >= max_episode_length :\n",
    "            \n",
    "            # Calculate sum of the rewards\n",
    "        episode_rewards_sum = sum(episode_rewards)     \n",
    "        G.append(episode_rewards_sum)\n",
    "        total_G = np.sum(G)\n",
    "        maximumReturn = np.amax(G)\n",
    "        print(\"%f\" % (episode_rewards_sum), file=open(\"PPO_Prosthetic_5000.txt\", \"a\"))\n",
    "        if ep % 10 == 0:\n",
    "                \n",
    "            print(\"==========================================\")\n",
    "            print(\"Episode: \", ep)\n",
    "            print(\"Rewards: \", episode_rewards_sum)\n",
    "            print(\"Max reward so far: \", maximumReturn)\n",
    "            # Mean reward\n",
    "            total_reward_mean = np.divide(total_G, ep+1)\n",
    "            G_mean.append(total_reward_mean)\n",
    "            print(\"Mean Reward\", total_reward_mean)\n",
    "            print(\"%f\" % (total_reward_mean), file=open(\"PPO_MEAN_Prosthetic_5000.txt\", \"a\"))\n",
    "                     \n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    \n",
    "    \n",
    "print('Finished.')\n",
    "plt.xlabel('episdes')\n",
    "plt.ylabel('reword')\n",
    "plt.plot(G)   \n",
    "plt.savefig('PPO_prosthetic_5000', dpi = 1000)\n",
    "\n",
    "\n",
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns')\n",
    "plt.xlabel('Number of episodes/10')\n",
    "\n",
    "plt.savefig(\"ReturnsAverage_VS_Episodes_PPO_prosthetic_5000\", dpi = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a59e43885000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PPO_Prosthetic_Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G)\n",
    "plt.ylabel('Returns')\n",
    "plt.xlabel('Number of episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(G_mean)\n",
    "plt.ylabel('Average of Returns ')\n",
    "plt.xlabel('Number of episodes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenSimEnv_V2)",
   "language": "python",
   "name": "opensimenv_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
